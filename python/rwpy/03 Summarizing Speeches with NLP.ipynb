{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Speeches with Natural Language Processing\n",
    "> NLP applications can summarize news feeds, analyze legal contracts, research patents, study financial markets, capture corporate knowledge, and produce study guides.\n",
    "\n",
    "1. Use Python's Natural Language Toolkit (NLTK) to generate a summary of MLK's \"I Have a Dream\" speech.\n",
    "2. Use a streamlined alternative (`gensim`) to summarize Admiral McRaven's \"Make Your Bed\" speech.\n",
    "\n",
    "## Project 3: I have a dream... to summarize speeches!\n",
    "There are two approaches to summarizing text: extraction and abstraction. \n",
    "\n",
    "Extraction uses weighting functions to rank sentences by perceived importance. Word importance is a function of word use. Think of it like using a highlighter to manually select keywords and sentences. \n",
    "\n",
    "Abstraction relies on deeper comprehension to produce human-like paraphrasing, including creating completely new sentences. These algorithms require complicated deep learning methods and sophisticated language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUMMARY:\n",
      "From every mountainside, let freedom ring.\n",
      "Let freedom ring from Lookout Mountain in Tennessee!\n",
      "Let freedom ring from every hill and molehill in Mississippi.\n",
      "Let freedom ring from the curvaceous slopes of California!\n",
      "Let freedom ring from the snow capped Rockies of Colorado!\n",
      "But one hundred years later the Negro is still not free.\n",
      "From the mighty mountains of New York, let freedom ring.\n",
      "From the prodigious hilltops of New Hampshire, let freedom ring.\n",
      "And I say to you today my friends, let freedom ring.\n",
      "I have a dream today.\n",
      "But not only there; let freedom ring from the Stone Mountain of Georgia!\n",
      "It is a dream deeply rooted in the American dream.\n",
      "Free at last!\n",
      "Thank God almighty, we're free at last!\"\n",
      "Now is the time to change racial injustice to the solid rock of brotherhood.\n",
      "We must not allow our creative protest to degenerate into physical violence.\n",
      "We must forever conduct our struggle on the high plane of dignity and discipline.\n",
      "This is the faith that I go back to the mount with.\n",
      "This is our hope.\n",
      "From the mighty Alleghenies of Pennsylvania!\n"
     ]
    }
   ],
   "source": [
    "# dream_summary.py\n",
    "# Scrape and summarize MLK's \"I Have a Dream\" speech.\n",
    "\n",
    "from collections import Counter # Keeps track of sentence scoring\n",
    "import re # Regex\n",
    "import requests # Downloads files and web pages\n",
    "import bs4 # Parses HTML\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def main():\n",
    "    url = 'http://www.analytictech.com/mb021/mlk.htm'\n",
    "    page = requests.get(url) # Fetches the url and assigns output to the `page` variable as a string.\n",
    "    page.raise_for_status() # No response if everything is okay.\n",
    "    soup = bs4.BeautifulSoup(page.text, 'html.parser')\n",
    "    p_elems = [element.text for element in soup.find_all('p')] # Find all text between HTML paragraph tags `<p>`.\n",
    "    # You could also write the previous line as `p_elems = soup.select('p')`.\n",
    "\n",
    "    speech = ''.join(p_elems)\n",
    "    speech = speech.replace(')mowing', 'knowing') # Typo in the original document\n",
    "    speech = re.sub('\\s+', ' ', speech) # Trim whitespace.\n",
    "    speech_edit = re.sub('[^a-zA-Z]', ' ', speech) # Remove everything that's not a letter by matching any character that isn't between the brackets.\n",
    "    speech_edit = re.sub('\\s+', ' ', speech_edit) # Trim whitespace created by the previous line.\n",
    "\n",
    "    while True:\n",
    "        max_words = input(\"Enter max words per sentence for summary: \") # Oxford Guide to Plain English suggests sentences between 15-20 words.\n",
    "        num_sents = input(\"Enter number of sentences for summary: \")\n",
    "        if max_words.isdigit() and num_sents.isdigit():\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nInput must be in whole numbers.\\n\")\n",
    "\n",
    "    speech_edit_no_stop = remove_stop_words(speech_edit)\n",
    "    word_freq = get_word_freq(speech_edit_no_stop)\n",
    "    sent_scores = score_sentences(speech, word_freq, max_words)\n",
    "\n",
    "    counts = Counter(sent_scores)\n",
    "    summary = counts.most_common(int(num_sents)) # Summary variable holds list of tuples. Sentence is at index [0], rank is at index [1].\n",
    "    print(\"\\nSUMMARY:\")\n",
    "    for i in summary:\n",
    "        print(i[0])\n",
    "\n",
    "def remove_stop_words(speech_edit):\n",
    "    \"\"\"Remove stop words from string and return string.\"\"\"\n",
    "    stop_words = set(stopwords.words('english')) # Create a set of English stop words.\n",
    "    speech_edit_no_stop = '' # Assign empty string to hold edited speech without stopwords.\n",
    "    for word in nltk.word_tokenize(speech_edit): # speech_edit is a string in which each element is a letter. Tokenize them to words.\n",
    "        if word.lower() not in stop_words:\n",
    "            speech_edit_no_stop += word + ' ' # If the word is not in the stop_words set, concatenate it to a new string with a space.\n",
    "    return speech_edit_no_stop # Return the string of words not in the stop_list set.\n",
    "\n",
    "def get_word_freq(speech_edit_no_stop):\n",
    "    \"\"\"Return a dictionary of word frequency in a string.\"\"\"\n",
    "    word_freq = nltk.FreqDist(nltk.word_tokenize(speech_edit_no_stop.lower()))\n",
    "    return word_freq\n",
    "\n",
    "def score_sentences(speech, word_freq, max_words):\n",
    "    \"\"\"Return dictionary of sentence scores based on word frequency.\"\"\"\n",
    "    sent_scores = dict() # Start an empty dictionary called sent_scores to hold sentence scores.\n",
    "    sentences = nltk.sent_tokenize(speech)\n",
    "    for sent in sentences: # Loop through the sentences\n",
    "        sent_scores[sent] = 0 # Update the sent_scores dictionary, assigning the sentence as the key, and setting its initial count to 0\n",
    "        words = nltk.word_tokenize(sent.lower()) # Tokenize the (lowercase) sentence to count word frequency. Lowercase to maintain compatibility with word_freq dictionary.\n",
    "        sent_word_count = len(words)\n",
    "        if sent_word_count <= int(max_words):\n",
    "            for word in words:\n",
    "                if word in word_freq.keys():\n",
    "                    sent_scores[sent] += word_freq[word]\n",
    "            sent_scores[sent] = sent_scores[sent] / sent_word_count # Normalize the count by dividing score by sentence length\n",
    "    return sent_scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 4: Summarizing speeches with `gensim`.\n",
    "`gensim` is an open source NLP library using statistical machine learning. (`gensim` stands for \"generate similar.\") It evaluates sentences by semantic similarity. The sentence most like the others is considered the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'has_pattern' from 'gensim.utils' (/opt/homebrew/lib/python3.9/site-packages/gensim/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/graftoncook/Documents/python/rwpy/03 Summarizing Speeches with NLP.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/graftoncook/Documents/python/rwpy/03%20Summarizing%20Speeches%20with%20NLP.ipynb#ch0000003?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbs4\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/graftoncook/Documents/python/rwpy/03%20Summarizing%20Speeches%20with%20NLP.ipynb#ch0000003?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/graftoncook/Documents/python/rwpy/03%20Summarizing%20Speeches%20with%20NLP.ipynb#ch0000003?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m \u001b[39mimport\u001b[39;00m summarize \u001b[39m# Gensim removed summarize from versions > gensim==3.8.3. Use `pip install gensim==3.8.3` to maintain this functionality.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/graftoncook/Documents/python/rwpy/03%20Summarizing%20Speeches%20with%20NLP.ipynb#ch0000003?line=9'>10</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://jamesclear.com/great-speeches/make-your-bed-by-admiral-william-h-mcraven\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/graftoncook/Documents/python/rwpy/03%20Summarizing%20Speeches%20with%20NLP.ipynb#ch0000003?line=10'>11</a>\u001b[0m page \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/summarization/__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/__init__.py?line=1'>2</a>\u001b[0m \u001b[39m# bring model classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msummarizer\u001b[39;00m \u001b[39mimport\u001b[39;00m summarize, summarize_corpus  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m      <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/__init__.py?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mkeywords\u001b[39;00m \u001b[39mimport\u001b[39;00m keywords  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m      <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/__init__.py?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmz_entropy\u001b[39;00m \u001b[39mimport\u001b[39;00m mz_keywords\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/summarization/summarizer.py:58\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/summarizer.py?line=55'>56</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[1;32m     <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/summarizer.py?line=56'>57</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpagerank_weighted\u001b[39;00m \u001b[39mimport\u001b[39;00m pagerank_weighted \u001b[39mas\u001b[39;00m _pagerank\n\u001b[0;32m---> <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/summarizer.py?line=57'>58</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtextcleaner\u001b[39;00m \u001b[39mimport\u001b[39;00m clean_text_by_sentences \u001b[39mas\u001b[39;00m _clean_text_by_sentences\n\u001b[1;32m     <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/summarizer.py?line=58'>59</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommons\u001b[39;00m \u001b[39mimport\u001b[39;00m build_graph \u001b[39mas\u001b[39;00m _build_graph\n\u001b[1;32m     <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/summarizer.py?line=59'>60</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommons\u001b[39;00m \u001b[39mimport\u001b[39;00m remove_unreachable_nodes \u001b[39mas\u001b[39;00m _remove_unreachable_nodes\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/summarization/textcleaner.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/textcleaner.py?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msyntactic_unit\u001b[39;00m \u001b[39mimport\u001b[39;00m SyntacticUnit\n\u001b[1;32m     <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/textcleaner.py?line=23'>24</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparsing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocess_documents\n\u001b[0;32m---> <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/textcleaner.py?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m tokenize, has_pattern\n\u001b[1;32m     <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/textcleaner.py?line=25'>26</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msix\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmoves\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mrange\u001b[39m\n\u001b[1;32m     <a href='file:///opt/homebrew/lib/python3.9/site-packages/gensim/summarization/textcleaner.py?line=26'>27</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'has_pattern' from 'gensim.utils' (/opt/homebrew/lib/python3.9/site-packages/gensim/utils.py)"
     ]
    }
   ],
   "source": [
    "# bed_summary.py\n",
    "# Scrape and summarize Adm. William McRaven's \"Make Your Bed\" speech.\n",
    "# Note to self: This speech is plain text from a more sophisticated webpage than MLK's speech. Is the scraping process different?\n",
    "# This works as a standalone .py file. I'm not sure why it won't work in Jupyter.\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.summarization import summarize # Gensim removed summarize from versions > gensim==3.8.3. Use `pip install gensim==3.8.3` to maintain this functionality.\n",
    "\n",
    "url = 'https://jamesclear.com/great-speeches/make-your-bed-by-admiral-william-h-mcraven'\n",
    "page = requests.get(url)\n",
    "page.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(page.text, 'html.parser')\n",
    "p_elems = [element.text for element in soup.find_all('p')]\n",
    "\n",
    "speech = ' '.join(p_elems)\n",
    "\n",
    "# Summarize the speech\n",
    "print(\"\\nSummary of Make Your Bed speech:\")\n",
    "summary = summarize(speech, word_count=225) # Summarize also allows a `ratio` option. E.g., `ratio=0.01` would produce a summary whose length is 1% of the original document.\n",
    "sentences = sent_tokenize(summary)\n",
    "sents = set(sentences) # Sets are unordered, so the summary arrangement may change with multiple runs.\n",
    "print(' '.join(sents)) # Don't combine print and summarize. It may duplicate sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5: Summarizing Text with Word Clouds\n",
    "I'll come back to this one."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
