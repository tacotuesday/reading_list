{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 16: Counting Vowels.\n",
    "The objective of the first project is to Write a Python program that counts the number of syllables in an English word or phrase. Every vowel sound is a syllable. Even though some vowel sounds are silent and some combine to lengthen the sound, exhaustive corpera exist which catalog the vowel sounds that are useful for us. The primary steps of this program are:\n",
    "1. Download a large corpus with syllable-count information.\n",
    "2. Compare the syllable-count corpus to the haiku-training corpus and identify all the words missing from the syllable-count corpus.\n",
    "3. Build a dictionary of the missing words and their syllable counts.\n",
    "4. Write a program that uses both the syllable-count corpus and the missing-words dictionary to count syllables in the training corpus.\n",
    "5. Write a program that checks the syllable-counting program against updates of the training corpus.\n",
    "\n",
    "This program uses the [Natural Language Toolkit](https://www.nltk.org) to access the Carnegie Mellon University Pronouncing Dictionary (CMUdict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download() # This doesn't work very well in Jupyter, but I wanted to leave it in anyway. Use the Python interpreter for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMUdict breaks words into sets of phonemes (perceptually distinct units of sound) and marks vowels for lexical stress using numbers (0, 1, and 2). You can use these numbers to identify the vowels in a word because every vowel is marked with one *and only one* of these numbers. Words with multiple pronounciations (e.g. \"aged\" versus \"agèd\") contain nested lists that return more than one syllable count.\n",
    "\n",
    "### Managing Missing Words\n",
    "The problem with corpera is that they inevitably miss words that you may want to use. In this case, there will also be some Japanese romanizations misinterpreted as English words with a different syllable count, like *sake*. We must make sure the words in our haiku training corpus (`train.txt`, from the [book's Github repo](https://github.com/rlvaugh/Impractical_Python_Projects)) are also in CMUdict. (Download `train.txt` to the same root as this Jupyter notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "exceptions:\n",
      "swordhand\n",
      "colour\n",
      "hibiscus\n",
      "stretchings\n",
      "spiritless\n",
      "deepener\n",
      "cloudbanks\n",
      "tendrilled\n",
      "samisen\n",
      "beholders\n",
      "asakura\n",
      "samuri\n",
      "battlers\n",
      "persimmons\n",
      "froglings\n",
      "ridgelines\n",
      "furue\n",
      "yowl\n",
      "windless\n",
      "lichened\n",
      "woodcutter\n",
      "whippoorwill\n",
      "nightingales\n",
      "treeline\n",
      "evenfall\n",
      "wintery\n",
      "cumulus\n",
      "tendrils\n",
      "archways\n",
      "carven\n",
      "dragonfly\n",
      "treehouse\n",
      "priestling\n",
      "camellia\n",
      "pattering\n",
      "wisteria\n",
      "fie\n",
      "oranged\n",
      "scatters\n",
      "skims\n",
      "storks\n",
      "windblown\n",
      "watersplash\n",
      "inuyasha\n",
      "cloudbank\n",
      "dusky\n",
      "nursemaid\n",
      "bathwater\n",
      "atsuta\n",
      "moonrise\n",
      "shadeless\n",
      "paperweights\n",
      "creepers\n",
      "foregather\n",
      "morningglory\n",
      "petaled\n",
      "dewdrop\n",
      "mooing\n",
      "\n",
      "Number of unique words in haiku corpus = 1523\n",
      "Number of words in corpus not in cmudict = 58\n",
      "cmudict membership = 96.2%\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3369: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# missingWordsFinder.py\n",
    "# This program checks train.txt entries for membership in CMUdict.\n",
    "import sys\n",
    "from string import punctuation\n",
    "import pprint\n",
    "import json\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "cmudict = cmudict.dict() # Carnegie Mellon University Pronouncing Dictionary\n",
    "\n",
    "def main():\n",
    "    haiku = load_haiku('train.txt')\n",
    "    exceptions = cmudict_missing(haiku)\n",
    "    build_dict = input(\"\\nManually build an exceptions dictionary (y/n)? \\n\")\n",
    "    if build_dict.lower() == 'n':\n",
    "        sys.exit()\n",
    "    else:\n",
    "        missing_words_dict = make_exceptions_dict(exceptions)\n",
    "        save_exceptions(missing_words_dict) # I originally forgot this line and had to diff with the source code from Github. :/\n",
    "\n",
    "def load_haiku(filename):\n",
    "    \"\"\"Open and return training corpus of haiku as a set.\"\"\"\n",
    "    with open(filename) as in_file:\n",
    "        haiku = set(in_file.read().replace('-', ' ').split()) # Load as a set to avoid repeats. Replace hyphens with spaces.\n",
    "        return haiku\n",
    "\n",
    "def cmudict_missing(word_set):\n",
    "    \"\"\"Find and return words in word set missing from cmudict.\"\"\"\n",
    "    exceptions = set() # Start an empty set to hold missing words.\n",
    "    for word in word_set:\n",
    "        word = word.lower().strip(punctuation)\n",
    "        if word.endswith(\"'s\") or word.endswith(\"’s\"):\n",
    "            word = word[:-2]\n",
    "        if word not in cmudict:\n",
    "            exceptions.add(word)\n",
    "    print(\"\\nexceptions:\")\n",
    "    print(*exceptions, sep='\\n')\n",
    "    print(\"\\nNumber of unique words in haiku corpus = {}\".format(len(word_set)))\n",
    "    print(\"Number of words in corpus not in cmudict = {}\".format(len(exceptions)))\n",
    "    membership = (1 - (len(exceptions) / len(word_set))) * 100\n",
    "    print(\"cmudict membership = {:.1f}{}\".format(membership, '%'))\n",
    "    return exceptions\n",
    "\n",
    "def make_exceptions_dict(exceptions_set):\n",
    "    \"\"\"Return dictionary of words and syllable counts from a set of words.\"\"\"\n",
    "    missing_words = {} # Assign an empty dictionary to missing_words\n",
    "    print(\"Input # syllables in word. Mistakes can be corrected at end. \\n\")\n",
    "    for word in exceptions_set:\n",
    "        while True:\n",
    "            num_sylls = input(\"Enter number of syllables in {}: \".format(word))\n",
    "            if num_sylls.isdigit():\n",
    "                break\n",
    "            else:\n",
    "                print(\"                   Not a valid answer!\", file=sys.stderr)\n",
    "            missing_words[word] = int(num_sylls)\n",
    "        print()\n",
    "        pprint.pprint(missing_words, width=1)\n",
    "\n",
    "        print(\"\\nMake Changes to Dictionary Before Saving?\")\n",
    "        print(\"\"\"\n",
    "        0 - Exit & Save\n",
    "        1 - Add a Word or Change a Syllable Count\n",
    "        2 - Remove a Word\n",
    "        \"\"\")\n",
    "\n",
    "        while True:\n",
    "            choice = input(\"\\nEnter choice: \")\n",
    "            if choice == '0':\n",
    "                break\n",
    "            elif choice == '1':\n",
    "                word = input(\"\\nWord to add or change: \")\n",
    "                missing_words[word] = int(input(\"Enter number of syllables in {}: \".format(word)))\n",
    "            elif choice == '2':\n",
    "                word = input(\"\\nEnter word to delete: \")\n",
    "                missing_words.pop(word, None)\n",
    "            \n",
    "        print(\"\\nNew words or syllable changes:\")\n",
    "        pprint.pprint(missing_words, width=1)\n",
    "\n",
    "        return missing_words\n",
    "\n",
    "def save_exceptions(missing_words):\n",
    "    \"\"\"Save exceptions dictionary as json file.\"\"\"\n",
    "    json_string = json.dumps(missing_words) # Serializes the missing_words dictionary into a string). Serializing is the process of converting data into a more transmittable string or storable format.\n",
    "    f = open('missing_words.json', 'w')\n",
    "    f.write(json_string)\n",
    "    f.close()\n",
    "    print(\"\\nFile saved as missing_words.json\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `missing_words.json`, we can write the code to count syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllable Counter\n",
      "The number of syllables in samuri is: 3\n",
      "\n",
      "Syllable Counter\n",
      "The number of syllables in moon is: 1\n",
      "\n",
      "Syllable Counter\n",
      "The number of syllables in sword is: 1\n",
      "\n",
      "Syllable Counter\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3369: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# count_syllables.py\n",
    "\n",
    "import sys\n",
    "from string import punctuation\n",
    "import json\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# load dictionary of words in haiku corpus but not in cmudict\n",
    "with open('missing_words.json') as f:\n",
    "    missing_words = json.load(f)\n",
    "\n",
    "cmudict = cmudict.dict() # Turns the CMUdict corpus into a dictionary.\n",
    "\n",
    "def count_syllables(words):\n",
    "    \"\"\"Use corpera to count syllables in English word or phrase.\"\"\"\n",
    "    # prep words for cmudict corpus\n",
    "    words = words.replace('-', ' ')\n",
    "    words = words.lower().split()\n",
    "    num_sylls = 0\n",
    "    for word in words:\n",
    "        word = word.strip(punctuation)\n",
    "        if word.endswith(\"'s\") or word.endswith(\"’s\"):\n",
    "            word = word[:-2]\n",
    "        if word in missing_words:\n",
    "            num_sylls += missing_words[word]\n",
    "        else:\n",
    "            for phonemes in cmudict[word][0]: # Refer to the first value in case the word has multiple pronounciations.\n",
    "                for phoneme in phonemes:\n",
    "                    if phoneme[-1].isdigit():\n",
    "                        num_sylls += 1\n",
    "    return num_sylls\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"Syllable Counter\")\n",
    "        word = input(\"Enter word or phrase; else press Enter to Exit: \")\n",
    "        if word == '':\n",
    "            sys.exit()\n",
    "        try:\n",
    "            num_syllables = count_syllables(word)\n",
    "            print(\"The number of syllables in {} is: {}\".format(word, num_syllables))\n",
    "            print()\n",
    "        except KeyError:\n",
    "            print(\"Word not found. Try again.\\n\", file=sys.stderr)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to add a new haiku to the corpus, but want to be aware of any words that aren't in CMUdict or your exceptions dictionary. The program below will automatically count the syllables in each word in your training corpus and display any word(s) on which it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing words:  []\n"
     ]
    }
   ],
   "source": [
    "# test_count_syllables_w_full_corpus.py\n",
    "# Make sure to include a separate count_syllables.py file in the same directory as this notebook.\n",
    "\n",
    "import sys\n",
    "import count_syllables\n",
    "\n",
    "with open('train.txt') as in_file:\n",
    "    words = set(in_file.read().split())\n",
    "\n",
    "missing = []\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        num_syllables = count_syllables.count_syllables(word)\n",
    "        ##print(word, num_syllables, end='\\n') # Uncomment to see word counts\n",
    "    except KeyError:\n",
    "        missing.append(word)\n",
    "\n",
    "print(\"Missing words: \", missing, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 17: Writing Haiku with Markov Chain Analysis\n",
    "\n",
    "> When applied to letters in words, a Markov model is a mathematical model that calculates a letter's probability of occurrence based on the previous *k* consecutive letters, where *k* is an integer. A *model of order 2* means that the probability of a letter occurring depends on the two letters that precede it.\n",
    "\n",
    "Markov models store every occurrence of a word as a separate duplicate value. You may get a list item like `'the': ['clouds', 'moon', 'moon']`: in this case, the odds of selecting `moon` versus `clouds` are 2:1. On the other hand, the model automatically screens rare or impossible combinations. A Markov model of order two generates lists that look more like `'the moon':  ['a', 'therefore']`. The size of *k* determines the novelty of the output: `0` results in random output based on the word's frequency in the corpus; `3` or more in the haiku corpus would result in plagiarism.\n",
    "\n",
    "The program will seed the haiku with a random word from the corpus; use a Markov model of order 1 to select the second word; then use order-2 models for each subsequent word. The author uses *ghost prefixes* in the case of syllable overrun: the program appends the model-2 output of a random two-word prefix.\n",
    "\n",
    "### Scaffolding and Debugging\n",
    "*Scaffolding* is temporary code written to help develop programs that is deleted from the final code. One common form of scaffolding is using the `print()` statement to check the return of a function or calculation. Other helpful pieces of scaffolding include value or variable type, dataset length, and incremental calculation results. Scaffolding can be troublesome if you accidentally comment out or delete a `print()` statement that you actually need in the end. \n",
    "\n",
    "An alternative to scaffolding is using the `logging` module, which reports on what your program is doing and can write permanent logfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - letter & count = s-0\n",
      "DEBUG - letter & count = c-0\n",
      "DEBUG - letter & count = a-1\n",
      "DEBUG - letter & count = r-1\n",
      "DEBUG - letter & count = e-2\n",
      "DEBUG - letter & count = c-2\n",
      "DEBUG - letter & count = r-2\n",
      "DEBUG - letter & count = o-3\n",
      "DEBUG - letter & count = w-3\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(levelname)s - %(message)s') # Set debugging information and format.\n",
    "\n",
    "word = 'scarecrow'\n",
    "VOWELS = 'aeiouy'\n",
    "num_vowels = 0\n",
    "for letter in word:\n",
    "    if letter in VOWELS:\n",
    "        num_vowels += 1\n",
    "    logging.debug('letter & count = %s-%s', letter, num_vowels) # Convert nonstring objects to strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output in the code above uses string formatting `%s`. Date and time are shown using `format='%(asctime)s'`. To disable the `logging` messages, insert `logging.disable(logging.CRITICAL)` under `import logging` and comment/uncomment as desired. `logging.disable()` suppresses all messages at or below the designated level; `CRITICAL` is the highest level, so passing it to `logging.disable()` turns all messages off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markov_haiku.py\n",
    "\n",
    "# Setup\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict # defaultdict builds a dictionary from a list by automatically creating a new key\n",
    "from count_syllables import count_syllables\n",
    "\n",
    "logging.disable(logging.CRITICAL) # comment out to enable debugging messages\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s')\n",
    "\n",
    "def load_training_file(file):\n",
    "    \"\"\"Return text file as a string.\"\"\"\n",
    "    with open(file) as f:\n",
    "        raw_haiku = f.read()\n",
    "        return raw_haiku\n",
    "\n",
    "def prep_training(raw_haiku):\n",
    "    \"\"\"Load string, remove newline, split words on spaces, and return list.\"\"\"\n",
    "    corpus = raw_haiku.replace('\\n', ' ').split()\n",
    "    return corpus\n",
    "\n",
    "# Building the one- and two-order Markov models.\n",
    "def map_word_to_word(corpus):\n",
    "    \"\"\"Load list & use dictionary to map word to word that follows.\"\"\"\n",
    "    limit = len(corpus) - 1\n",
    "    dict1_to_1 = defaultdict(list)\n",
    "    for index, word in enumerate(corpus):\n",
    "        if index < limit:\n",
    "            suffix = corpus[index + 1]\n",
    "            dict1_to_1[word].append(suffix)\n",
    "    logging.debug(\"map_word_to_word results for \\\"sake\\\" = %s\\n\", dict1_to_1['sake'])\n",
    "    return dict1_to_1\n",
    "\n",
    "def map_2_words_to_word(corpus):\n",
    "    \"\"\"Load list & use dictionary to map word-pair to trailing word.\"\"\"\n",
    "    limit = len(corpus) - 2\n",
    "    dict2_to_1 = defaultdict(list)\n",
    "    for index, word in enumerate(corpus):\n",
    "        if index < limit:\n",
    "            key = word + ' ' + corpus[index + 1]\n",
    "            suffix = corpus[index + 2]\n",
    "            dict2_to_1[key].append(suffix)\n",
    "    logging.debug(\"map_2_words_to_word results for \\\"sake jug\\\" = %s\\n\", dict2_to_1['sake jug'])\n",
    "    return dict2_to_1\n",
    "\n",
    "# Choosing a random word.\n",
    "\n",
    "def random_word(corpus):\n",
    "    \"\"\"Return random word and syllable count from training corpus.\"\"\"\n",
    "    word = random.choice(corpus)\n",
    "    num_syls = count_syllables(word)\n",
    "    if num_syls > 4:\n",
    "        random_word(corpus)\n",
    "    else:\n",
    "        logging.debug(\"random word & syllables = %s %s\\n\", word, num_syls)\n",
    "        return (word, num_syls)\n",
    "\n",
    "# Applying the Markov models\n",
    "def word_after_single(prefix, suffix_map_1, current_syls, target_syls):\n",
    "    \"\"\"Return all acceptable words in a corpus that follow a single word.\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
